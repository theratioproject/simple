/* 
	Copyright (c) 2018 Azeez Adewale <azeezadewale98@gmail.com"> 
	MIT License Copyright (c) 2018 SIMPLE

*/

/* 
 * @Filename - spider.sim
 * @Author - Azeez Adewale
 * @Date - 06 April 2018
 * @Time - 02:56 AM
 */
 /*
		WEBSPIDER
		This program is able to download any webpage and save it on the
		computer system. Webpage content can be display in console or 
		downloaded as file into specified folder.
		Cloning a website can take time but this utility program can do just
		that for you just send it your url and your save location that all you 
		need to get the website in your local host
		
		
		#failing
		-> does not currently download the css (fixed)
		-> does not currently download the scripts
		
 */
 
call "simple/internet/curl/CurlCore.sim"
call "simple/io/Directory.sim"
call "simple/internet/Download.sim"
call "simple/core/String.sim"

cmdlen = lengthOf(cmdparams)
execType = -1 #do nothing
cloneTo = "./"
specifysavepath = false 
link = "" 
dLoc = "index.html"
isattrib = false 

block main() 
	for a = 0 to cmdlen 
		if (stringStartsWith(cmdparams[a],'-'))
				if stringEndsWith(cmdparams[a],'help') or stringEndsWith(cmdparams[a],'h') help() 
				elif stringEndsWith(cmdparams[a],'p') 
					execType = 2  if cmdlen > a link = cmdparams[a++] break 
					else printError("The URL is expected after the -p flag") end 
				elif !stringEndsWith(cmdparams[a],'id') and stringEndsWith(cmdparams[a],'d') 
					execType = 1 
					if cmdlen > a link = cmdparams[a+1] 
					dLoc = cloneTo+'\'+dLoc  
					break
					else printError("The URL is expected after the -d flag") end
				elif stringEndsWith(cmdparams[a],'try') 
					execType = 4  if cmdlen > a link = cmdparams[a++] break 
					else printError("The URL is expected after the -p flag") end 
				elif stringEndsWith(cmdparams[a],'link') 
					isattrib = true attrib = 'href="'
					if cmdlen > a link = cmdparams[a++] break  end
				elif stringEndsWith(cmdparams[a],'image') || stringEndsWith(cmdparams[a],'img') 
					isattrib = true attrib = 'src="'
					if cmdlen > a link = cmdparams[a++] break  end
				elif stringEndsWith(cmdparams[a],'id') 
					isattrib = true attrib = 'id="'
					if cmdlen > a link = cmdparams[a++] break  end
				elif stringEndsWith(cmdparams[a],'style') 
					isattrib = true attrib = 'style="'
					if cmdlen > a link = cmdparams[a++] break  end
				elif stringEndsWith(cmdparams[a],'class') 
					isattrib = true attrib = 'class="'
					if cmdlen > a link = cmdparams[a++] break  end
				elif stringStartsWith(toUpperCase(cmdparams[a]),'-A/') 
					isattrib = true 
						 attrib = removeString(cmdparams[a],"-A/") + '="' 
					if cmdlen > a link = cmdparams[a++] break  end
				elif stringEndsWith(cmdparams[a],'clone') or stringEndsWith(cmdparams[a],'c')
					execType = 0
					if cmdlen > a link = cmdparams[a++] cloneTo = cmdparams[a+1]+'/' end 
					break
				elif stringStartsWith(toUpperCase(cmdparams[a]),'-F/') specifysavepath  = true cloneTo = removeString(cmdparams[a],"-F/")+ '/' 
				elif stringStartsWith(toUpperCase(cmdparams[a]),'-H/') link = removeString(cmdparams[a],"-H/")+ '/' 
				elif stringStartsWith(toUpperCase(cmdparams[a]),'-O/') oldhost = removeString(cmdparams[a],"-O/") 
				elif stringStartsWith(toUpperCase(cmdparams[a]),'-N/') newhost = removeString(cmdparams[a],"-N/") 
				elif stringEndsWith(cmdparams[a],'ch') execType = 3  
				else printError("Invalid parameter : "+cmdparams[a]) end
			end
	end
	if link == "" help() end
	ws = new WebSpider(link)
	if execType == 1 #download
		if (lastStringAfterChar(link, '/') == "" ) dLoc = "index.html" 
		else dLoc = replaceString(lastStringAfterChar(link, '/'), lastStringAfterChar(link, '.'), 'html') end
		display "downloading "+link+" into "+dLoc ws.downloadLocation(dLoc)+crlf 
	elif execType == 0
		if (stringStartsWith(link,"https://"))  
		cloneTo += replaceString(removeString(link, "https://"), '/', '_') 
		elif (stringStartsWith(link,"http://"))  cloneTo += replaceString(removeString(link, "http://"), '/', '_') end
		display "Clonning "+link+" into "+cloneTo+crlf
		dir = new Directory(cloneTo) if (dir.exists() == false) display "creating directory "+cloneTo+crlf dir.create() end
		ws.cloneInto(cloneTo)
	else if isattrib ws.setAttrib(attrib) end
	end
	ws.execute(execType) 
	/*if (execType == 0 )
	else
		if (lastStringAfterChar(link, '/') == "" ) dLoc = "index.html" 
		else dLoc = replaceString(lastStringAfterChar(link, '/'), lastStringAfterChar(link, '.'), 'html') end
		
		if (execType == 1 )  end 
		if (execType == 3 ) ws.newHost(oldhost,newhost) 
		else ws.setAttrib(attrib) end
		
	end*/
	#display crlf+"DONE"
	
block help() header() 
	display "This program is use to prepare your server for web development with
simple cgi and web module. Visit simple environments page for more included features
Usage:  ./spider ([OPTIONS]) ([FLAGS]) [PATH]

Flags :
	-H/<url>		url, link or address of the website | must start with http or https
	-F/<savefolder>		savefolder | folder to save the result
	>> <filename>		save the console output into filename instead
	
The OPTIONS include :
	-cure 					prepare your local server to allow simple cgi	
	-revert					in-case of corrupt .conf revert the old .conf file
	-T/<simple-webpage>			print the result of you simple webpage
	
	(Website Clone) ~ clone a whole website or webpage
	-c	<url>	<savefolder>		clone the website from the url into the savefolder
	-clone	<url>	<savefolder>		clone the website from the url into the savefolder
	-p		<url>		print the entire source of the webpage from the url
	-O/<oldhost> -N/<newhost> H<url>	download the webpage from the url and replace all instance of the oldhost with the newhost
	
	(Webpage Attributes) ~ scrap attribute from a webpage 
	-image 		<url>		scrap all the image links from the url
	-link		<url>		scrap all the url/links from the url
	-id 		<url>		fetch all the elements id from the url
	-class 		<url>		fetch all the elements class from the url
	-style 		<url>		fetch all the elements styles from the url
	-A/<attr> 	<url>		specify the attribute you wish to scrap from the url"
	__exit__

block printError(msg) header() 
	display "SPIDER : "+msg + crlf
	display "======================================================="
	__exit__
	
block header()
	display crlf+"=======================================================
	SPIDER : web spider Program : 2018
	This program is part of simple environments 
======================================================="+crlf
	
	
class WebSpider 

	baseLink = "http://simple-lang.sourceforge.net"
	curl = null
	downloadPath = ""
	newHost = ""
	oldHost = ""
	sbl = ""
	isclonning = false
	indent = "->"
	$attrib = ""
	crawedList = []
 
	block WebSpider(link)
		baseLink = link
		sbl = removeLastChar(link)
		
	block execute(type)
		if (type==1)
			savePage(baseLink,downloadPath)
		elif (type==2)
			print()
		elif (type==3)
			replaceHost(oldHost,newHost,baseLink)
		elif (type==4)
			tryPage()
		elif (type==0)
			isclonning = true 
			#if lastStringAfterChar(baseLink, '/') = "" 
			#savePage(baseLink,downloadPath+"/index.sim") 
			#add(crawedList,downloadPath+"/index.sim") end 
			clone('',downloadPath)
		else 
			printAttrib($attrib)
		end
		
	block setAttrib(attrib)
		$attrib = attrib
		
	block downloadLocation(loc)
		downloadPath = loc
		
	block newHost(old,host)
		oldHost = old
		newHost = host 
		
	block cloneInto(cloneto)
		downloadPath = cloneto
		
	block getLinksInContent(content)
		return getAttributeInContent(content,'href="')
		
	block getAttributeInContent(content,attrib)
			links = [] content = replaceString(content, attrib, '`')
			retval = stringSplit(content, "`")
			relen = lengthOf(retval)
			for a = 1 to relen
				rere = stringSplit(retval[a], '"')
				if (isclonning)
					if (stringStartsWith(rere[1],'> Parent Directory'))
						continue
						#we definitely don't want to re-download parent folder of url 
					end
				end
				add(links,rere[0]) 
				if isattrib display rere[0] + crlf end
			end
			if isattrib __exit__ end
			return links
	
	private
	
		enterTheDragon = false
	
		block clone(link,path)
			if path != downloadPath path = downloadPath + '\'+ link end
			indent += "`" if link == "" link = baseLink else link = baseLink+link end
			curl = new CurlCore(link) 
			pageContent = curl.getStringOutput() 
			curl.flush() 
			value = getLinksInContent(pageContent) vlen = lengthOf(value) 
			for a = 0 to vlen
				if (!stringEquals(value[a], "",true) && !stringStartsWith(value[a], "#"))
					if (stringStartsWith(value[a], baseLink))
						if not checkList(crawedList,value[a]) treatEasy(value[a],path) end
					else
						if stringStartsWith(value[a], "http") display "~~ Avoiding External Link : "+value[a]+crlf 
						if not enterTheDragon continue end end
						if not checkList(crawedList,value[a]) treatEasy(link+value[a],path) else display " <<"+crlf end
					end
				end
			end
			
		block treatEasy(link,dpath)
			isaveFolder = "" o = link add(crawedList,link)
			if (stringEndsWith(link,'/')) 
			else o = removeString(link,lastStringAfterChar(link, '/')) end
			isaveFolder = replaceString(o,sbl,downloadPath)
			dir = new Directory(isaveFolder) 
			if (dir.exists()) else display "creating directory : "+dir.getName()+crlf dir.create() end 
			if (!stringEndsWith(link, "zip") && !stringEndsWith(link, "rar")) 
				path = dir.getPath()+'\'+lastStringAfterChar(replaceString(link,'\','/'), '/') 
				if (stringEndsWith(path, "\") ) 
					if (lengthOf(dpath)==lengthOf(removeLastChar(path))) 
						display "~~~~~~skipping index file "+crlf
						return 
					end
					path = path+"/index.html" #it a folder appending 'index.html' ;)
					#we are currently skipping folder
				end
				display indent+" downloading " +getName(link)+" to "+getName(dir.getPath())+crlf
				#path = replaceString(path,'.sim', '.html')
				#link = replaceString(link,'.sim', '.html')
				savePage(link,path) 
				if stringEndsWith(path, "html")
					link = removeString(link,baseLink)
					link = replaceString(link,'\',char(7635)) 
					link = removeString(link,char(7635))
					clone(link,dir.getPath())
					display "we going for "+replaceString(link,'\','/')+ crlf
				end
				
			end
			
		block checkList(llist,value)
			llen = lengthOf(llist)
			for a = 0 to llen
				if llist[a] = value return true end
				#display "~~~~~~ : "+llist[a]+crlf+"    ~~ : "+value+crlf
			end
			return false
		
		block replaceHost(ohost,nhost,filepath)
			tFile = new File(filepath)
			newContent = replaceString(tFile.readFileAsString(), ohost, nhost)
			tFile.write(newContent)
		
		block savePage(base,path)
			d = new Download(base)
			d.saveLocation(path)
			d.execute()
			#replaceHost(baseLink,'./',path)
			#replaceHost('.sim','.html',path)
			
		block printAttrib(type)
			curl = new CurlCore(baseLink)
			content = curl.getStringOutput() curl.flush()
			display getAttributeInContent(content,type)
			
		block print()
			curl = new CurlCore(baseLink)
			display curl.getStringOutput() 
			curl.flush()
			
		block getName(oldName)
			name = replaceString(oldName,'\','/')
			name = lastStringAfterChar(name,'/')
			return name
			
		block tryPage()
			#__exec('simple "'+link+'"')
			
			